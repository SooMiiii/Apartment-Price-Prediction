{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 랜덤 시드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "park = pd.read_csv('parkInfo.csv')\n",
    "schools = pd.read_csv('schoolinfo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           index   area_m2  contract_year_month  contract_day  contract_type  \\\n",
      "15            15   84.9342               201907            31              2   \n",
      "16            16   84.9342               201907            31              2   \n",
      "28            28  146.4005               201911            21              2   \n",
      "29            29  146.4005               201911            21              2   \n",
      "33            33   84.9342               201912            14              2   \n",
      "...          ...       ...                  ...           ...            ...   \n",
      "1801197  1801197  101.9088               202308            22              2   \n",
      "1801198  1801198  114.9285               202308            28              1   \n",
      "1801199  1801199  114.9285               202308            28              1   \n",
      "1801210  1801210  114.9285               202310            26              2   \n",
      "1801211  1801211  114.9285               202310            26              2   \n",
      "\n",
      "         floor  built_year   latitude   longitude  age  deposit  \n",
      "15           7        2016  36.965423  127.048779    3  18000.0  \n",
      "16           7        2016  36.965423  127.048779    3  18000.0  \n",
      "28           5        2016  36.965423  127.048779    3  37000.0  \n",
      "29           5        2016  36.965423  127.048779    3  37000.0  \n",
      "33          14        2016  36.965423  127.048779    3  19000.0  \n",
      "...        ...         ...        ...         ...  ...      ...  \n",
      "1801197     11        2010  37.528394  126.659398   13  33000.0  \n",
      "1801198     18        2010  37.528394  126.659398   13  30000.0  \n",
      "1801199     18        2010  37.528394  126.659398   13  30000.0  \n",
      "1801210      9        2010  37.528394  126.659398   13  39000.0  \n",
      "1801211      9        2010  37.528394  126.659398   13  39000.0  \n",
      "\n",
      "[162417 rows x 11 columns]\n",
      "중복된 행의 수: 162417\n"
     ]
    }
   ],
   "source": [
    "# 'index'를 제외한 모든 열에 대해 중복 확인\n",
    "columns_to_check = [col for col in train.columns if col != 'index']\n",
    "duplicates = train.duplicated(subset=columns_to_check, keep=False)\n",
    "\n",
    "# 중복된 행 출력\n",
    "print(train[duplicates])\n",
    "\n",
    "# 중복된 행의 수 출력\n",
    "print(f\"중복된 행의 수: {duplicates.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터셋 크기: 1801228\n",
      "중복 제거 후 데이터셋 크기: 1717611\n",
      "제거된 행의 수: 83617\n"
     ]
    }
   ],
   "source": [
    "# 중복 제거\n",
    "train_no_duplicates = train.drop_duplicates(subset=columns_to_check)\n",
    "\n",
    "print(f\"원본 데이터셋 크기: {len(train)}\")\n",
    "print(f\"중복 제거 후 데이터셋 크기: {len(train_no_duplicates)}\")\n",
    "print(f\"제거된 행의 수: {len(train) - len(train_no_duplicates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복제거 데이터셋 크기: 1717611\n",
      "1/100 후 데이터셋 크기: 17176\n",
      "제거된 행의 수: 1700435\n"
     ]
    }
   ],
   "source": [
    "train_sample = train_no_duplicates.sample(frac=0.01, random_state=42)\n",
    "print(f\"중복제거 데이터셋 크기: {len(train_no_duplicates)}\")\n",
    "print(f\"1/100 후 데이터셋 크기: {len(train_sample)}\")\n",
    "print(f\"제거된 행의 수: {len(train_no_duplicates) - len(train_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 5399.236570026839\n",
      "예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train_df = train_no_duplicates\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=2)\n",
    "\n",
    "# 예측 함수\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = 1 / (distances + 1e-5)\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 예측 수행 함수\n",
    "def make_predictions(data_scaled, k):\n",
    "    distances, indices = tree.query(data_scaled, k=k)\n",
    "    predictions = []\n",
    "    for i in range(len(data_scaled)):\n",
    "        neighbor_indices = indices[i]\n",
    "        neighbor_distances = distances[i]\n",
    "        neighbors = train_df.iloc[neighbor_indices]\n",
    "        pred = predict_deposit(neighbors, neighbor_distances)\n",
    "        predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# Validation 예측\n",
    "k = 5\n",
    "val_predictions = make_predictions(val_scaled, k)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = make_predictions(test_scaled, k)\n",
    "\n",
    "# 결과 저장\n",
    "submission_df = pd.DataFrame({\n",
    "    'index': test_df['index'],\n",
    "    'deposit': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 8816.349132933661\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train_df = train_sample\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=2)\n",
    "\n",
    "# 예측 함수\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = 1 / (distances + 1e-5)\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 예측 수행 함수\n",
    "def make_predictions(data_scaled, k):\n",
    "    distances, indices = tree.query(data_scaled, k=k)\n",
    "    predictions = []\n",
    "    for i in range(len(data_scaled)):\n",
    "        neighbor_indices = indices[i]\n",
    "        neighbor_distances = distances[i]\n",
    "        neighbors = train_df.iloc[neighbor_indices]\n",
    "        pred = predict_deposit(neighbors, neighbor_distances)\n",
    "        predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# Validation 예측\n",
    "k = 5\n",
    "val_predictions = make_predictions(val_scaled, k)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = make_predictions(test_scaled, k)\n",
    "\n",
    "# 결과 저장\n",
    "submission_df = pd.DataFrame({\n",
    "    'index': test_df['index'],\n",
    "    'deposit': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 8580.527569380753\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train_df = train_sample\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = [1, 1, 2, 0.5, 0.5, 1]  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=2)\n",
    "\n",
    "# 예측 함수\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = 1 / (distances + 1e-5)\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 예측 수행 함수\n",
    "def make_predictions(data_scaled, k):\n",
    "    distances, indices = tree.query(data_scaled, k=k)\n",
    "    predictions = []\n",
    "    for i in range(len(data_scaled)):\n",
    "        neighbor_indices = indices[i]\n",
    "        neighbor_distances = distances[i]\n",
    "        neighbors = train_df.iloc[neighbor_indices]\n",
    "        pred = predict_deposit(neighbors, neighbor_distances)\n",
    "        predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# Validation 예측\n",
    "k = 5\n",
    "val_predictions = make_predictions(val_scaled, k)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = make_predictions(test_scaled, k)\n",
    "\n",
    "# 결과 저장\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'index': test_df['index'],\n",
    "#     'deposit': test_predictions\n",
    "# })\n",
    "\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 8537.947125057197\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train_df = train_sample\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = [1, 1, 5, 0.5, 0.5, 0.5]  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=2)\n",
    "\n",
    "# 예측 함수\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = 1 / (distances + 1e-5)\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 예측 수행 함수\n",
    "def make_predictions(data_scaled, k):\n",
    "    distances, indices = tree.query(data_scaled, k=k)\n",
    "    predictions = []\n",
    "    for i in range(len(data_scaled)):\n",
    "        neighbor_indices = indices[i]\n",
    "        neighbor_distances = distances[i]\n",
    "        neighbors = train_df.iloc[neighbor_indices]\n",
    "        pred = predict_deposit(neighbors, neighbor_distances)\n",
    "        predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# Validation 예측\n",
    "k = 5\n",
    "val_predictions = make_predictions(val_scaled, k)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = make_predictions(test_scaled, k)\n",
    "\n",
    "# 결과 저장\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'index': test_df['index'],\n",
    "#     'deposit': test_predictions\n",
    "# })\n",
    "\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 5190.820037476034\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_mae\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Test 예측\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmake_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 34\u001b[0m, in \u001b[0;36mmake_predictions\u001b[0;34m(data_scaled, k)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_predictions\u001b[39m(data_scaled, k):\n\u001b[0;32m---> 34\u001b[0m     distances, indices \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_scaled)):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train_df = train_no_duplicates\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = [1, 1, 5, 0.5, 0.5, 0.5]  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=2)\n",
    "\n",
    "# 예측 함수\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = 1 / (distances + 1e-5)\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 예측 수행 함수\n",
    "def make_predictions(data_scaled, k):\n",
    "    distances, indices = tree.query(data_scaled, k=k)\n",
    "    predictions = []\n",
    "    for i in range(len(data_scaled)):\n",
    "        neighbor_indices = indices[i]\n",
    "        neighbor_distances = distances[i]\n",
    "        neighbors = train_df.iloc[neighbor_indices]\n",
    "        pred = predict_deposit(neighbors, neighbor_distances)\n",
    "        predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# Validation 예측\n",
    "k = 5\n",
    "val_predictions = make_predictions(val_scaled, k)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = make_predictions(test_scaled, k)\n",
    "\n",
    "# 결과 저장\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'index': test_df['index'],\n",
    "#     'deposit': test_predictions\n",
    "# })\n",
    "\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 병렬처리 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 5190.968640296472\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 데이터 로드\n",
    "train_df = train_no_duplicates\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=40)  # leaf_size를 증가시켜 성능 향상\n",
    "\n",
    "# 예측 함수\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = 1 / (distances + 1e-5)\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 데이터 포인트에 대한 예측 함수\n",
    "def predict_single(data_point, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, k) for data_point in data_scaled)\n",
    "\n",
    "# Validation 예측\n",
    "k = 5\n",
    "val_predictions = make_predictions_parallel(val_scaled, k)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = make_predictions_parallel(test_scaled, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 병렬처리 동적 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 5197.313491807083\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "train_df = train_no_duplicates\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=40)\n",
    "\n",
    "# 예측 함수\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = 1 / (distances + 1e-5)\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 예측 함수\n",
    "def predict_single(data_point, min_k, max_k):\n",
    "    distances, indices = tree.query([data_point], k=max_k)\n",
    "    k = min(max_k, max(min_k, int(np.sum(distances < np.median(distances)))))\n",
    "    neighbor_indices = indices[0][:k]\n",
    "    neighbor_distances = distances[0][:k]\n",
    "    neighbors = train_df.iloc[neighbor_indices]\n",
    "    return predict_deposit(neighbors, neighbor_distances)\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, min_k, max_k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, min_k, max_k) for data_point in data_scaled)\n",
    "\n",
    "# Validation 예측\n",
    "min_k = 3\n",
    "max_k = 10\n",
    "val_predictions = make_predictions_parallel(val_scaled, min_k, max_k)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = make_predictions_parallel(test_scaled, min_k, max_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이상치 처리 병렬 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 8552.217652688742\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train_df = train_sample\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=40)  # leaf_size를 증가시켜 성능 향상\n",
    "\n",
    "# 예측 함수\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = 1 / (distances + 1e-5)\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 데이터 포인트에 대한 예측 함수\n",
    "def predict_single(data_point, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, k) for data_point in data_scaled)\n",
    "\n",
    "# 이상치 처리 함수\n",
    "def handle_outliers_log(predictions, threshold=3):\n",
    "    log_predictions = np.log1p(predictions)\n",
    "    median = np.median(log_predictions)\n",
    "    mad = np.median(np.abs(log_predictions - median))\n",
    "    lower_bound = median - threshold * mad\n",
    "    upper_bound = median + threshold * mad\n",
    "    clipped_log_predictions = np.clip(log_predictions, lower_bound, upper_bound)\n",
    "    return np.expm1(clipped_log_predictions)\n",
    "\n",
    "# Validation 예측\n",
    "k = 5\n",
    "val_predictions = make_predictions_parallel(val_scaled, k)\n",
    "val_predictions = val_predictions = handle_outliers_log(val_predictions, threshold=5)  # 임계값을 높임\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = make_predictions_parallel(test_scaled, k)\n",
    "test_predictions = handle_outliers_log(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE (Region-based): 8545.55447674819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 데이터 로드\n",
    "train_df = train_sample\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=40)  # leaf_size를 증가시켜 성능 향상\n",
    "\n",
    "# 예측 함수\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = 1 / (distances + 1e-5)\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 데이터 포인트에 대한 예측 함수\n",
    "def predict_single(data_point, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, k) for data_point in data_scaled)\n",
    "\n",
    "# 지역별 이상치 처리 함수\n",
    "def handle_outliers_by_region(predictions, latitudes, longitudes, threshold=3):\n",
    "    # predictions를 numpy 배열로 변환\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # 간단한 지역 클러스터링\n",
    "    coords = np.column_stack((latitudes, longitudes))\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42).fit(coords)\n",
    "    regions = kmeans.predict(coords)\n",
    "    \n",
    "    cleaned_predictions = np.zeros_like(predictions)\n",
    "    for region in np.unique(regions):\n",
    "        region_mask = (regions == region)\n",
    "        region_predictions = predictions[region_mask]\n",
    "        region_cleaned = handle_outliers_log(region_predictions, threshold)\n",
    "        cleaned_predictions[region_mask] = region_cleaned\n",
    "    \n",
    "    return cleaned_predictions\n",
    "\n",
    "# Validation 예측\n",
    "k = 5\n",
    "val_predictions = make_predictions_parallel(val_scaled, k)\n",
    "\n",
    "# 지역별 이상치 처리 적용\n",
    "val_predictions_region = handle_outliers_by_region(val_predictions, val_df['latitude'], val_df['longitude'], threshold=5)\n",
    "\n",
    "val_mae_region = mean_absolute_error(val_df['deposit'], val_predictions_region)\n",
    "\n",
    "print(f\"Validation MAE (Region-based): {val_mae_region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 8537.869233380277\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "train_df = train_sample\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=40)\n",
    "\n",
    "# 예측 함수\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = 1 / (distances + 1e-5)\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 예측 함수\n",
    "def predict_single(data_point, min_k, max_k):\n",
    "    distances, indices = tree.query([data_point], k=max_k)\n",
    "    k = min(max_k, max(min_k, int(np.sum(distances < np.median(distances)))))\n",
    "    neighbor_indices = indices[0][:k]\n",
    "    neighbor_distances = distances[0][:k]\n",
    "    neighbors = train_df.iloc[neighbor_indices]\n",
    "    return predict_deposit(neighbors, neighbor_distances)\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, min_k, max_k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, min_k, max_k) for data_point in data_scaled)\n",
    "\n",
    "# Validation 예측\n",
    "min_k = 3\n",
    "max_k = 10\n",
    "val_predictions = make_predictions_parallel(val_scaled, min_k, max_k)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = make_predictions_parallel(test_scaled, min_k, max_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 8405.48183286097\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train_df = train_sample\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=40)\n",
    "\n",
    "# 예측 함수\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = 1 / (distances + 1e-5)\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 예측 함수\n",
    "def predict_single(data_point, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, k) for data_point in data_scaled)\n",
    "\n",
    "# 앙상블 예측 함수\n",
    "def ensemble_predictions(data_scaled, k_values):\n",
    "    all_predictions = Parallel(n_jobs=-1)(\n",
    "        delayed(make_predictions_parallel)(data_scaled, k) for k in k_values\n",
    "    )\n",
    "    return np.mean(all_predictions, axis=0)\n",
    "\n",
    "# k 값 리스트 정의\n",
    "k_values = [3, 5, 7, 9]\n",
    "\n",
    "# Validation 예측\n",
    "val_predictions = ensemble_predictions(val_scaled, k_values)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = ensemble_predictions(test_scaled, k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 8486.101253090092\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "train_df = train_sample\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축\n",
    "tree = BallTree(train_scaled, leaf_size=40)\n",
    "\n",
    "# 예측 함수 (거리에 따른 가중치 함수 조정)\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = np.exp(-distances)  # 지수 함수 사용\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 예측 함수\n",
    "def predict_single(data_point, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, k) for data_point in data_scaled)\n",
    "\n",
    "# 앙상블 예측 함수\n",
    "def ensemble_predictions(data_scaled, k_values):\n",
    "    all_predictions = Parallel(n_jobs=-1)(\n",
    "        delayed(make_predictions_parallel)(data_scaled, k) for k in k_values\n",
    "    )\n",
    "    return np.mean(all_predictions, axis=0)\n",
    "\n",
    "# k 값 리스트 정의\n",
    "k_values = [3, 5, 7, 9]\n",
    "\n",
    "# Validation 예측\n",
    "val_predictions = ensemble_predictions(val_scaled, k_values)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = ensemble_predictions(test_scaled, k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StandardScaler, RobustScaler, MinMaxScaler / 로그변환 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler without Log Transform - Validation MAE: 8405.48183286097\n",
      "StandardScaler with Log Transform - Validation MAE: 8468.876950218077\n",
      "RobustScaler without Log Transform - Validation MAE: 8584.839449059366\n",
      "RobustScaler with Log Transform - Validation MAE: 8540.002242918164\n",
      "MinMaxScaler without Log Transform - Validation MAE: 8566.245673670523\n",
      "MinMaxScaler with Log Transform - Validation MAE: 8771.50220388082\n",
      "\n",
      "Results sorted by MAE:\n",
      "StandardScaler without Log: 8405.48183286097\n",
      "StandardScaler with Log: 8468.876950218077\n",
      "RobustScaler with Log: 8540.002242918164\n",
      "MinMaxScaler without Log: 8566.245673670523\n",
      "RobustScaler without Log: 8584.839449059366\n",
      "MinMaxScaler with Log: 8771.50220388082\n",
      "\n",
      "Best method: StandardScaler without Log with MAE: 8405.48183286097\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 데이터 로드 (이전 코드에서 가정)\n",
    "train_df = train_sample\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 로그 변환 함수\n",
    "def log_transform(df):\n",
    "    return df.apply(lambda x: np.log1p(x) if x.min() >= 0 else x)\n",
    "\n",
    "# 스케일링 및 예측 함수\n",
    "def scale_and_predict(scaler, apply_log=False):\n",
    "    if apply_log:\n",
    "        train_scaled = log_transform(train_df[features])\n",
    "        val_scaled = log_transform(val_df[features])\n",
    "        test_scaled = log_transform(test_df[features])\n",
    "    else:\n",
    "        train_scaled = train_df[features]\n",
    "        val_scaled = val_df[features]\n",
    "        test_scaled = test_df[features]\n",
    "    \n",
    "    train_scaled = scaler.fit_transform(train_scaled)\n",
    "    val_scaled = scaler.transform(val_scaled)\n",
    "    test_scaled = scaler.transform(test_scaled)\n",
    "    \n",
    "    # 특성 가중치 조정\n",
    "    weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])\n",
    "    train_scaled = train_scaled * weights\n",
    "    val_scaled = val_scaled * weights\n",
    "    test_scaled = test_scaled * weights\n",
    "    \n",
    "    # BallTree 구축\n",
    "    tree = BallTree(train_scaled, leaf_size=40)\n",
    "    \n",
    "    # 예측 함수\n",
    "    def predict_deposit(neighbors, distances):\n",
    "        weights = 1 / (distances + 1e-5)\n",
    "        weighted_deposits = neighbors['deposit'] * weights\n",
    "        return np.sum(weighted_deposits) / np.sum(weights)\n",
    "    \n",
    "    # 단일 예측 함수\n",
    "    def predict_single(data_point, k):\n",
    "        distances, indices = tree.query([data_point], k=k)\n",
    "        neighbors = train_df.iloc[indices[0]]\n",
    "        return predict_deposit(neighbors, distances[0])\n",
    "    \n",
    "    # 병렬 예측 수행 함수\n",
    "    def make_predictions_parallel(data_scaled, k):\n",
    "        return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, k) for data_point in data_scaled)\n",
    "    \n",
    "    # 앙상블 예측 함수\n",
    "    def ensemble_predictions(data_scaled, k_values):\n",
    "        all_predictions = Parallel(n_jobs=-1)(\n",
    "            delayed(make_predictions_parallel)(data_scaled, k) for k in k_values\n",
    "        )\n",
    "        return np.mean(all_predictions, axis=0)\n",
    "    \n",
    "    # k 값 리스트 정의\n",
    "    k_values = [3, 5, 7, 9]\n",
    "    \n",
    "    # Validation 예측\n",
    "    val_predictions = ensemble_predictions(val_scaled, k_values)\n",
    "    \n",
    "    # Validation MAE 계산\n",
    "    val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "    \n",
    "    return val_mae\n",
    "\n",
    "# 각 스케일러에 대해 MAE 계산\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    # 로그 변환 없이\n",
    "    mae_without_log = scale_and_predict(scaler, apply_log=False)\n",
    "    print(f\"{name} without Log Transform - Validation MAE: {mae_without_log}\")\n",
    "    results[f\"{name} without Log\"] = mae_without_log\n",
    "    \n",
    "    # 로그 변환 적용\n",
    "    mae_with_log = scale_and_predict(scaler, apply_log=True)\n",
    "    print(f\"{name} with Log Transform - Validation MAE: {mae_with_log}\")\n",
    "    results[f\"{name} with Log\"] = mae_with_log\n",
    "\n",
    "# 결과 정렬\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nResults sorted by MAE:\")\n",
    "for method, mae in sorted_results:\n",
    "    print(f\"{method}: {mae}\")\n",
    "\n",
    "print(f\"\\nBest method: {sorted_results[0][0]} with MAE: {sorted_results[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 거리 계산 방식을 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Validation MAE: 8486.101253090092\n",
      "Manhattan Validation MAE: 8031.291888515132\n",
      "Minkowski (p=3) Validation MAE: 8742.976038886232\n",
      "\n",
      "Best performing distance metric: Manhattan with MAE: 8031.291888515132\n",
      "\n",
      "Test predictions using Manhattan distance metric have been generated.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import BallTree, KDTree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 데이터 로드\n",
    "train_df = train_sample\n",
    "test_df = test\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# 예측 함수 (거리에 따른 가중치 함수 조정)\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = np.exp(-distances)  # 지수 함수 사용\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 예측 함수\n",
    "def predict_single(tree, data_point, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(tree, data_scaled, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(tree, data_point, k) for data_point in data_scaled)\n",
    "\n",
    "# 앙상블 예측 함수\n",
    "def ensemble_predictions(tree, data_scaled, k_values):\n",
    "    all_predictions = Parallel(n_jobs=-1)(\n",
    "        delayed(make_predictions_parallel)(tree, data_scaled, k) for k in k_values\n",
    "    )\n",
    "    return np.mean(all_predictions, axis=0)\n",
    "\n",
    "# k 값 리스트 정의\n",
    "k_values = [3, 5, 7, 9]\n",
    "\n",
    "# 각 거리 메트릭에 대한 예측 및 MAE 계산 함수\n",
    "def predict_and_calculate_mae(tree, name):\n",
    "    val_predictions = ensemble_predictions(tree, val_scaled, k_values)\n",
    "    val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "    print(f\"{name} Validation MAE: {val_mae}\")\n",
    "    return val_mae\n",
    "\n",
    "# 유클리드 거리 (기본 BallTree)\n",
    "euclidean_tree = BallTree(train_scaled, leaf_size=40, metric='euclidean')\n",
    "euclidean_mae = predict_and_calculate_mae(euclidean_tree, \"Euclidean\")\n",
    "\n",
    "# 맨해튼 거리\n",
    "manhattan_tree = BallTree(train_scaled, leaf_size=40, metric='manhattan')\n",
    "manhattan_mae = predict_and_calculate_mae(manhattan_tree, \"Manhattan\")\n",
    "\n",
    "# 민코프스키 거리 (p=3)\n",
    "minkowski_tree = BallTree(train_scaled, leaf_size=40, metric='minkowski', p=3)\n",
    "minkowski_mae = predict_and_calculate_mae(minkowski_tree, \"Minkowski (p=3)\")\n",
    "\n",
    "# 결과 비교\n",
    "results = {\n",
    "    \"Euclidean\": euclidean_mae,\n",
    "    \"Manhattan\": manhattan_mae,\n",
    "    \"Minkowski (p=3)\": minkowski_mae\n",
    "}\n",
    "\n",
    "best_metric = min(results, key=results.get)\n",
    "print(f\"\\nBest performing distance metric: {best_metric} with MAE: {results[best_metric]}\")\n",
    "\n",
    "# 최적의 거리 메트릭을 사용하여 테스트 데이터에 대한 예측\n",
    "best_tree = locals()[f\"{best_metric.lower().split()[0]}_tree\"]\n",
    "test_predictions = ensemble_predictions(best_tree, test_scaled, k_values)\n",
    "\n",
    "print(f\"\\nTest predictions using {best_metric} distance metric have been generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맨해튼 거리방식으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 특성 선택\u001b[39;00m\n\u001b[1;32m     14\u001b[0m features \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeposit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 데이터 로드\n",
    "train_df = pd.read_csv('merged_data.csv')\n",
    "test_df = pd.read_csv\n",
    "\n",
    "# 특성 선택\n",
    "features = train_df.columns.drop(['deposit', 'index']).tolist()\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축 (맨해튼 거리 사용)\n",
    "tree = BallTree(train_scaled, leaf_size=40, metric='manhattan')\n",
    "\n",
    "# 예측 함수 (거리에 따른 가중치 함수 조정)\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = np.exp(-distances)  # 지수 함수 사용\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 예측 함수\n",
    "def predict_single(data_point, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, k) for data_point in data_scaled)\n",
    "\n",
    "# 앙상블 예측 함수\n",
    "def ensemble_predictions(data_scaled, k_values):\n",
    "    all_predictions = Parallel(n_jobs=-1)(\n",
    "        delayed(make_predictions_parallel)(data_scaled, k) for k in k_values\n",
    "    )\n",
    "    return np.mean(all_predictions, axis=0)\n",
    "\n",
    "# k 값 리스트 정의\n",
    "k_values = [3, 5, 7, 9]\n",
    "\n",
    "# Validation 예측\n",
    "val_predictions = ensemble_predictions(val_scaled, k_values)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = ensemble_predictions(test_scaled, k_values)\n",
    "\n",
    "# 결과 저장\n",
    "submission_df = pd.DataFrame({\n",
    "    'index': test_df['index'],\n",
    "    'deposit': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m k_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m9\u001b[39m]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Validation 예측\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m val_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Validation MAE 계산\u001b[39;00m\n\u001b[1;32m     67\u001b[0m val_mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeposit\u001b[39m\u001b[38;5;124m'\u001b[39m], val_predictions)\n",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m, in \u001b[0;36mensemble_predictions\u001b[0;34m(data_scaled, k_values)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mensemble_predictions\u001b[39m(data_scaled, k_values):\n\u001b[0;32m---> 55\u001b[0m     all_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_predictions_parallel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk_values\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(all_predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 데이터 로드\n",
    "merged_data = pd.read_csv('merged_data_cleaned.csv')\n",
    "\n",
    "# Train과 Test 데이터 분리\n",
    "train_df = merged_data[merged_data['deposit'] != 0].copy()\n",
    "test_df = merged_data[merged_data['deposit'] == 0].copy()\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축 (맨해튼 거리 사용)\n",
    "tree = BallTree(train_scaled, leaf_size=40, metric='manhattan')\n",
    "\n",
    "# 예측 함수 (거리에 따른 가중치 함수 조정)\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = np.exp(-distances)  # 지수 함수 사용\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 예측 함수\n",
    "def predict_single(data_point, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, k) for data_point in data_scaled)\n",
    "\n",
    "# 앙상블 예측 함수\n",
    "def ensemble_predictions(data_scaled, k_values):\n",
    "    all_predictions = Parallel(n_jobs=-1)(\n",
    "        delayed(make_predictions_parallel)(data_scaled, k) for k in k_values\n",
    "    )\n",
    "    return np.mean(all_predictions, axis=0)\n",
    "\n",
    "# k 값 리스트 정의\n",
    "k_values = [3, 5, 7, 9]\n",
    "\n",
    "# Validation 예측\n",
    "val_predictions = ensemble_predictions(val_scaled, k_values)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = ensemble_predictions(test_scaled, k_values)\n",
    "\n",
    "# 결과 저장\n",
    "submission_df = pd.DataFrame({\n",
    "    'index': test_df.index,\n",
    "    'deposit': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['nearest_park_distance', 'park_count_500m', 'total_park_area_500m', 'park_count_1000m', 'total_park_area_1000m', 'park_count_2000m', 'total_park_area_2000m', 'weighted_park_score', 'avg_distance_5_parks', 'park_distance_skewness', 'park_distance_kurtosis', 'nearest_large_park_distance', 'large_park_count_3km', 'large_park_count_5km', 'large_park_count_10km', 'total_large_park_area_10km', 'nearest_subway_distance_km', 'school_count_within_1km', 'closest_elementary_distance', 'closest_middle_distance', 'closest_high_distance', 'deposit_mean', 'interest_rate', 'interest_rate_diff'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(train_df[features])\n\u001b[1;32m     38\u001b[0m val_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(val_df[features])\n\u001b[0;32m---> 39\u001b[0m test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mtest_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 특성 가중치 조정 (모든 특성에 대해 동일한 가중치 1 적용)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(features))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['nearest_park_distance', 'park_count_500m', 'total_park_area_500m', 'park_count_1000m', 'total_park_area_1000m', 'park_count_2000m', 'total_park_area_2000m', 'weighted_park_score', 'avg_distance_5_parks', 'park_distance_skewness', 'park_distance_kurtosis', 'nearest_large_park_distance', 'large_park_count_3km', 'large_park_count_5km', 'large_park_count_10km', 'total_large_park_area_10km', 'nearest_subway_distance_km', 'school_count_within_1km', 'closest_elementary_distance', 'closest_middle_distance', 'closest_high_distance', 'deposit_mean', 'interest_rate', 'interest_rate_diff'] not in index\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 로드\n",
    "merged_data = pd.read_csv('merged_data.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# 'deposit' 열이 있는지 확인하고 있으면 train 데이터로, 없으면 test 데이터로 처리\n",
    "if 'deposit' in merged_data.columns:\n",
    "    train_df = merged_data[merged_data['deposit'].notna()].copy()\n",
    "    test_df = test_data.copy()\n",
    "else:\n",
    "    print(\"Warning: 'deposit' column not found in merged_data. Using all merged_data as training data.\")\n",
    "    train_df = merged_data.copy()\n",
    "    test_df = test_data.copy()\n",
    "\n",
    "# 불필요한 열 제거\n",
    "columns_to_drop = ['index'] if 'index' in train_df.columns else []\n",
    "train_df = train_df.drop(columns=columns_to_drop)\n",
    "test_df = test_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# 특성 선택\n",
    "features = [col for col in train_df.columns if col != 'deposit']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정 (모든 특성에 대해 동일한 가중치 1 적용)\n",
    "weights = np.ones(len(features))\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축 (맨해튼 거리 사용)\n",
    "tree = BallTree(train_scaled, leaf_size=40, metric='manhattan')\n",
    "\n",
    "# 예측 함수 (거리에 따른 가중치 함수 조정)\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = np.exp(-distances)  # 지수 함수 사용\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 예측 함수\n",
    "def predict_single(data_point, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, k) for data_point in data_scaled)\n",
    "\n",
    "# 앙상블 예측 함수\n",
    "def ensemble_predictions(data_scaled, k_values):\n",
    "    all_predictions = Parallel(n_jobs=-1)(\n",
    "        delayed(make_predictions_parallel)(data_scaled, k) for k in k_values\n",
    "    )\n",
    "    return np.mean(all_predictions, axis=0)\n",
    "\n",
    "# k 값 리스트 정의\n",
    "k_values = [3, 5, 7, 9]\n",
    "\n",
    "# Validation 예측\n",
    "val_predictions = ensemble_predictions(val_scaled, k_values)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = ensemble_predictions(test_scaled, k_values)\n",
    "\n",
    "# 결과 저장\n",
    "submission_df = pd.DataFrame({\n",
    "    'index': test_df['index'] if 'index' in test_df.columns else range(len(test_predictions)),\n",
    "    'deposit': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")\n",
    "\n",
    "# 특성 중요도 분석\n",
    "# 1. 상관관계 기반 중요도\n",
    "correlation_importance = train_df[features + ['deposit']].corr()['deposit'].abs().sort_values(ascending=False)\n",
    "\n",
    "# 2. 랜덤 포레스트 기반 중요도\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(train_scaled, train_df['deposit'])\n",
    "rf_importance = pd.Series(rf_model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n상관관계 기반 특성 중요도:\")\n",
    "print(correlation_importance)\n",
    "\n",
    "print(\"\\n랜덤 포레스트 기반 특성 중요도:\")\n",
    "print(rf_importance)\n",
    "\n",
    "# 중요도 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "correlation_importance[:10].plot(kind='bar')\n",
    "plt.title('Top 10 Features (Correlation-based Importance)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_importance.png')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "rf_importance[:10].plot(kind='bar')\n",
    "plt.title('Top 10 Features (Random Forest Importance)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rf_importance.png')\n",
    "\n",
    "print(\"특성 중요도 그래프가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_m2</th>\n",
       "      <th>contract_year_month</th>\n",
       "      <th>contract_day</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>floor</th>\n",
       "      <th>built_year</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>age</th>\n",
       "      <th>deposit</th>\n",
       "      <th>...</th>\n",
       "      <th>large_park_count_10km</th>\n",
       "      <th>total_large_park_area_10km</th>\n",
       "      <th>nearest_subway_distance_km</th>\n",
       "      <th>school_count_within_1km</th>\n",
       "      <th>closest_elementary_distance</th>\n",
       "      <th>closest_middle_distance</th>\n",
       "      <th>closest_high_distance</th>\n",
       "      <th>deposit_mean</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>interest_rate_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.9981</td>\n",
       "      <td>201906</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2019</td>\n",
       "      <td>37.054314</td>\n",
       "      <td>127.045216</td>\n",
       "      <td>0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>3082215.1</td>\n",
       "      <td>0.716953</td>\n",
       "      <td>4</td>\n",
       "      <td>0.156120</td>\n",
       "      <td>0.465125</td>\n",
       "      <td>0.990855</td>\n",
       "      <td>31188.259433</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.9981</td>\n",
       "      <td>202003</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2019</td>\n",
       "      <td>37.054314</td>\n",
       "      <td>127.045216</td>\n",
       "      <td>1</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>3082215.1</td>\n",
       "      <td>0.716953</td>\n",
       "      <td>4</td>\n",
       "      <td>0.156120</td>\n",
       "      <td>0.465125</td>\n",
       "      <td>0.990855</td>\n",
       "      <td>32309.834287</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.9981</td>\n",
       "      <td>202003</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "      <td>37.054314</td>\n",
       "      <td>127.045216</td>\n",
       "      <td>1</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>3082215.1</td>\n",
       "      <td>0.716953</td>\n",
       "      <td>4</td>\n",
       "      <td>0.156120</td>\n",
       "      <td>0.465125</td>\n",
       "      <td>0.990855</td>\n",
       "      <td>32309.834287</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.3400</td>\n",
       "      <td>201907</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1986</td>\n",
       "      <td>36.964647</td>\n",
       "      <td>127.055847</td>\n",
       "      <td>33</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1877924.1</td>\n",
       "      <td>3.897280</td>\n",
       "      <td>4</td>\n",
       "      <td>0.214560</td>\n",
       "      <td>0.688047</td>\n",
       "      <td>0.644366</td>\n",
       "      <td>31786.283137</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.8100</td>\n",
       "      <td>201904</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1995</td>\n",
       "      <td>36.972390</td>\n",
       "      <td>127.084514</td>\n",
       "      <td>24</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2199482.1</td>\n",
       "      <td>2.039685</td>\n",
       "      <td>0</td>\n",
       "      <td>1.708489</td>\n",
       "      <td>2.197946</td>\n",
       "      <td>2.264822</td>\n",
       "      <td>30459.486563</td>\n",
       "      <td>2.04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867778</th>\n",
       "      <td>115.5101</td>\n",
       "      <td>202402</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>2010</td>\n",
       "      <td>37.528394</td>\n",
       "      <td>126.659398</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>10819839.1</td>\n",
       "      <td>1.483045</td>\n",
       "      <td>7</td>\n",
       "      <td>0.313129</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.224754</td>\n",
       "      <td>46357.076007</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867779</th>\n",
       "      <td>142.8738</td>\n",
       "      <td>202403</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>37.528394</td>\n",
       "      <td>126.659398</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>10819839.1</td>\n",
       "      <td>1.483045</td>\n",
       "      <td>7</td>\n",
       "      <td>0.313129</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.224754</td>\n",
       "      <td>46711.029696</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867780</th>\n",
       "      <td>142.8738</td>\n",
       "      <td>202403</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2010</td>\n",
       "      <td>37.528394</td>\n",
       "      <td>126.659398</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>10819839.1</td>\n",
       "      <td>1.483045</td>\n",
       "      <td>7</td>\n",
       "      <td>0.313129</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.224754</td>\n",
       "      <td>46711.029696</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867781</th>\n",
       "      <td>114.9285</td>\n",
       "      <td>202403</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>37.528394</td>\n",
       "      <td>126.659398</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>10819839.1</td>\n",
       "      <td>1.483045</td>\n",
       "      <td>7</td>\n",
       "      <td>0.313129</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.224754</td>\n",
       "      <td>46711.029696</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867782</th>\n",
       "      <td>115.5101</td>\n",
       "      <td>202403</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2010</td>\n",
       "      <td>37.528394</td>\n",
       "      <td>126.659398</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>10819839.1</td>\n",
       "      <td>1.483045</td>\n",
       "      <td>7</td>\n",
       "      <td>0.313129</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.224754</td>\n",
       "      <td>46711.029696</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867783 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          area_m2  contract_year_month  contract_day  contract_type  floor  \\\n",
       "0         84.9981               201906            25              2      9   \n",
       "1         84.9981               202003            26              2     20   \n",
       "2         84.9981               202003            28              2      8   \n",
       "3         59.3400               201907            15              2      1   \n",
       "4         59.8100               201904            12              2      6   \n",
       "...           ...                  ...           ...            ...    ...   \n",
       "1867778  115.5101               202402            27              0     17   \n",
       "1867779  142.8738               202403             2              0      4   \n",
       "1867780  142.8738               202403            16              1     13   \n",
       "1867781  114.9285               202403            22              1      2   \n",
       "1867782  115.5101               202403            22              1      7   \n",
       "\n",
       "         built_year   latitude   longitude  age  deposit  ...  \\\n",
       "0              2019  37.054314  127.045216    0  17000.0  ...   \n",
       "1              2019  37.054314  127.045216    1  23000.0  ...   \n",
       "2              2019  37.054314  127.045216    1  23000.0  ...   \n",
       "3              1986  36.964647  127.055847   33   5000.0  ...   \n",
       "4              1995  36.972390  127.084514   24   1800.0  ...   \n",
       "...             ...        ...         ...  ...      ...  ...   \n",
       "1867778        2010  37.528394  126.659398   14      0.0  ...   \n",
       "1867779        2010  37.528394  126.659398   14      0.0  ...   \n",
       "1867780        2010  37.528394  126.659398   14      0.0  ...   \n",
       "1867781        2010  37.528394  126.659398   14      0.0  ...   \n",
       "1867782        2010  37.528394  126.659398   14      0.0  ...   \n",
       "\n",
       "         large_park_count_10km  total_large_park_area_10km  \\\n",
       "0                           14                   3082215.1   \n",
       "1                           14                   3082215.1   \n",
       "2                           14                   3082215.1   \n",
       "3                            7                   1877924.1   \n",
       "4                            8                   2199482.1   \n",
       "...                        ...                         ...   \n",
       "1867778                     21                  10819839.1   \n",
       "1867779                     21                  10819839.1   \n",
       "1867780                     21                  10819839.1   \n",
       "1867781                     21                  10819839.1   \n",
       "1867782                     21                  10819839.1   \n",
       "\n",
       "         nearest_subway_distance_km  school_count_within_1km  \\\n",
       "0                          0.716953                        4   \n",
       "1                          0.716953                        4   \n",
       "2                          0.716953                        4   \n",
       "3                          3.897280                        4   \n",
       "4                          2.039685                        0   \n",
       "...                             ...                      ...   \n",
       "1867778                    1.483045                        7   \n",
       "1867779                    1.483045                        7   \n",
       "1867780                    1.483045                        7   \n",
       "1867781                    1.483045                        7   \n",
       "1867782                    1.483045                        7   \n",
       "\n",
       "         closest_elementary_distance  closest_middle_distance  \\\n",
       "0                           0.156120                 0.465125   \n",
       "1                           0.156120                 0.465125   \n",
       "2                           0.156120                 0.465125   \n",
       "3                           0.214560                 0.688047   \n",
       "4                           1.708489                 2.197946   \n",
       "...                              ...                      ...   \n",
       "1867778                     0.313129                 0.482436   \n",
       "1867779                     0.313129                 0.482436   \n",
       "1867780                     0.313129                 0.482436   \n",
       "1867781                     0.313129                 0.482436   \n",
       "1867782                     0.313129                 0.482436   \n",
       "\n",
       "         closest_high_distance  deposit_mean  interest_rate  \\\n",
       "0                     0.990855  31188.259433           1.92   \n",
       "1                     0.990855  32309.834287           1.63   \n",
       "2                     0.990855  32309.834287           1.63   \n",
       "3                     0.644366  31786.283137           1.94   \n",
       "4                     2.264822  30459.486563           2.04   \n",
       "...                        ...           ...            ...   \n",
       "1867778               0.224754  46357.076007           3.97   \n",
       "1867779               0.224754  46711.029696           4.00   \n",
       "1867780               0.224754  46711.029696           4.00   \n",
       "1867781               0.224754  46711.029696           4.00   \n",
       "1867782               0.224754  46711.029696           4.00   \n",
       "\n",
       "         interest_rate_diff  \n",
       "0                     -0.07  \n",
       "1                      0.08  \n",
       "2                      0.08  \n",
       "3                      0.02  \n",
       "4                       NaN  \n",
       "...                     ...  \n",
       "1867778                0.15  \n",
       "1867779                0.03  \n",
       "1867780                0.03  \n",
       "1867781                0.03  \n",
       "1867782                0.03  \n",
       "\n",
       "[1867783 rows x 35 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (1717611, 35)\n",
      "Test data shape: (150172, 35)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "merged_data = pd.read_csv('merged_data.csv')\n",
    "\n",
    "# Train과 Test 데이터 분리\n",
    "train_df = merged_data[merged_data['deposit'] != 0].copy()\n",
    "test_df = merged_data[merged_data['deposit'] == 0].copy()\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN 값이 있는 열:\n",
      "['index', 'park_distance_skewness', 'park_distance_kurtosis', 'interest_rate_diff']\n"
     ]
    }
   ],
   "source": [
    "# NaN 값 확인\n",
    "print(\"NaN 값이 있는 열:\")\n",
    "print(merged_data.columns[merged_data.isna().any()].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN 값이 있는 열과 해당 열의 NaN 비율:\n",
      "\n",
      "전체 데이터셋에서 NaN 값의 비율: 0.00%\n",
      "NaN 값이 하나 이상 있는 행의 비율: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 로드\n",
    "merged_data = pd.read_csv('merged_data_cleaned.csv')\n",
    "\n",
    "# NaN 값이 있는 열 확인\n",
    "nan_columns = merged_data.columns[merged_data.isna().any()].tolist()\n",
    "\n",
    "print(\"NaN 값이 있는 열과 해당 열의 NaN 비율:\")\n",
    "for col in nan_columns:\n",
    "    nan_percentage = (merged_data[col].isna().sum() / len(merged_data)) * 100\n",
    "    print(f\"{col}: {nan_percentage:.2f}%\")\n",
    "\n",
    "# 전체 데이터셋에서 NaN 값의 비율\n",
    "total_nan_percentage = (merged_data.isna().sum().sum() / (merged_data.shape[0] * merged_data.shape[1])) * 100\n",
    "print(f\"\\n전체 데이터셋에서 NaN 값의 비율: {total_nan_percentage:.2f}%\")\n",
    "\n",
    "# NaN 값이 있는 행의 비율\n",
    "rows_with_nan = merged_data[merged_data.isna().any(axis=1)]\n",
    "rows_with_nan_percentage = (len(rows_with_nan) / len(merged_data)) * 100\n",
    "print(f\"NaN 값이 하나 이상 있는 행의 비율: {rows_with_nan_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "남아있는 NaN 값:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "정제된 데이터 형태: (1867783, 34)\n",
      "\n",
      "정제된 데이터의 처음 몇 행:\n",
      "   area_m2  contract_year_month  contract_day  contract_type  floor  \\\n",
      "0  84.9981               201906            25              2      9   \n",
      "1  84.9981               202003            26              2     20   \n",
      "2  84.9981               202003            28              2      8   \n",
      "3  59.3400               201907            15              2      1   \n",
      "4  59.8100               201904            12              2      6   \n",
      "\n",
      "   built_year   latitude   longitude  age  deposit  ...  \\\n",
      "0        2019  37.054314  127.045216    0  17000.0  ...   \n",
      "1        2019  37.054314  127.045216    1  23000.0  ...   \n",
      "2        2019  37.054314  127.045216    1  23000.0  ...   \n",
      "3        1986  36.964647  127.055847   33   5000.0  ...   \n",
      "4        1995  36.972390  127.084514   24   1800.0  ...   \n",
      "\n",
      "   large_park_count_10km  total_large_park_area_10km  \\\n",
      "0                     14                   3082215.1   \n",
      "1                     14                   3082215.1   \n",
      "2                     14                   3082215.1   \n",
      "3                      7                   1877924.1   \n",
      "4                      8                   2199482.1   \n",
      "\n",
      "   nearest_subway_distance_km  school_count_within_1km  \\\n",
      "0                    0.716953                        4   \n",
      "1                    0.716953                        4   \n",
      "2                    0.716953                        4   \n",
      "3                    3.897280                        4   \n",
      "4                    2.039685                        0   \n",
      "\n",
      "   closest_elementary_distance  closest_middle_distance  \\\n",
      "0                     0.156120                 0.465125   \n",
      "1                     0.156120                 0.465125   \n",
      "2                     0.156120                 0.465125   \n",
      "3                     0.214560                 0.688047   \n",
      "4                     1.708489                 2.197946   \n",
      "\n",
      "   closest_high_distance  deposit_mean  interest_rate  interest_rate_diff  \n",
      "0               0.990855  31188.259433           1.92               -0.07  \n",
      "1               0.990855  32309.834287           1.63                0.08  \n",
      "2               0.990855  32309.834287           1.63                0.08  \n",
      "3               0.644366  31786.283137           1.94                0.02  \n",
      "4               2.264822  30459.486563           2.04                0.00  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "\n",
      "정제된 데이터가 'merged_data_cleaned.csv' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 로드\n",
    "merged_data = pd.read_csv('merged_data.csv')\n",
    "\n",
    "# index 열 제거\n",
    "merged_data = merged_data.drop('index', axis=1)\n",
    "\n",
    "# NaN 값을 중앙값으로 대체\n",
    "columns_to_fill = ['park_distance_skewness', 'park_distance_kurtosis', 'interest_rate_diff']\n",
    "for col in columns_to_fill:\n",
    "    merged_data[col] = merged_data[col].fillna(merged_data[col].median())\n",
    "\n",
    "# 결과를 새로운 데이터프레임에 저장\n",
    "merged_data_cleaned = merged_data.copy()\n",
    "\n",
    "# NaN 값이 모두 처리되었는지 확인\n",
    "remaining_nan = merged_data_cleaned.isna().sum()\n",
    "print(\"남아있는 NaN 값:\")\n",
    "print(remaining_nan[remaining_nan > 0])\n",
    "\n",
    "# 정보 출력\n",
    "print(\"\\n정제된 데이터 형태:\", merged_data_cleaned.shape)\n",
    "print(\"\\n정제된 데이터의 처음 몇 행:\")\n",
    "print(merged_data_cleaned.head())\n",
    "\n",
    "# 필요하다면 정제된 데이터를 CSV 파일로 저장\n",
    "merged_data_cleaned.to_csv('merged_data_cleaned.csv', index=False)\n",
    "print(\"\\n정제된 데이터가 'merged_data_cleaned.csv' 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN 값이 있는 열:\n",
      "[]\n",
      "\n",
      "처리 후 NaN 값이 있는 열:\n",
      "[]\n",
      "\n",
      "스케일링 후 NaN 값 확인:\n",
      "train_scaled NaN: False\n",
      "val_scaled NaN: False\n",
      "test_scaled NaN: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m k_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m9\u001b[39m]\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Validation 예측\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m val_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Validation MAE 계산\u001b[39;00m\n\u001b[1;32m     94\u001b[0m val_mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeposit\u001b[39m\u001b[38;5;124m'\u001b[39m], val_predictions)\n",
      "Cell \u001b[0;32mIn[32], line 82\u001b[0m, in \u001b[0;36mensemble_predictions\u001b[0;34m(data_scaled, k_values)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mensemble_predictions\u001b[39m(data_scaled, k_values):\n\u001b[0;32m---> 82\u001b[0m     all_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_predictions_parallel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk_values\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(all_predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 로드\n",
    "merged_data = pd.read_csv('merged_data_cleaned.csv')\n",
    "\n",
    "# NaN 값 확인\n",
    "print(\"NaN 값이 있는 열:\")\n",
    "print(merged_data.columns[merged_data.isna().any()].tolist())\n",
    "\n",
    "# NaN 값 처리\n",
    "for column in merged_data.columns:\n",
    "    if merged_data[column].isna().any():\n",
    "        if column != 'deposit':  # deposit 열은 NaN으로 둡니다 (테스트 데이터의 경우)\n",
    "            merged_data[column] = merged_data[column].fillna(merged_data[column].median())\n",
    "\n",
    "# 다시 한 번 NaN 값 확인\n",
    "print(\"\\n처리 후 NaN 값이 있는 열:\")\n",
    "print(merged_data.columns[merged_data.isna().any()].tolist())\n",
    "\n",
    "# Train과 Test 데이터 분리\n",
    "train_df = merged_data[merged_data['deposit'] != 0].copy()\n",
    "test_df = merged_data[merged_data['deposit'] == 0].copy()\n",
    "\n",
    "# 특성 선택\n",
    "features = [col for col in merged_data.columns if col not in ['deposit', 'index']]\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# NaN 값 다시 확인\n",
    "print(\"\\n스케일링 후 NaN 값 확인:\")\n",
    "print(\"train_scaled NaN:\", np.isnan(train_scaled).any())\n",
    "print(\"val_scaled NaN:\", np.isnan(val_scaled).any())\n",
    "print(\"test_scaled NaN:\", np.isnan(test_scaled).any())\n",
    "\n",
    "# 만약 스케일링 후에도 NaN 값이 있다면 0으로 대체\n",
    "train_scaled = np.nan_to_num(train_scaled)\n",
    "val_scaled = np.nan_to_num(val_scaled)\n",
    "test_scaled = np.nan_to_num(test_scaled)\n",
    "\n",
    "# 특성 가중치 조정 (모든 특성에 대해 동일한 가중치 1 적용)\n",
    "weights = np.ones(len(features))\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축 (맨해튼 거리 사용)\n",
    "tree = BallTree(train_scaled, leaf_size=40, metric='manhattan')\n",
    "\n",
    "# 예측 함수 (거리에 따른 가중치 함수 조정)\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = np.exp(-distances)  # 지수 함수 사용\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 예측 함수\n",
    "def predict_single(data_point, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, k) for data_point in data_scaled)\n",
    "\n",
    "# 앙상블 예측 함수\n",
    "def ensemble_predictions(data_scaled, k_values):\n",
    "    all_predictions = Parallel(n_jobs=-1)(\n",
    "        delayed(make_predictions_parallel)(data_scaled, k) for k in k_values\n",
    "    )\n",
    "    return np.mean(all_predictions, axis=0)\n",
    "\n",
    "# k 값 리스트 정의\n",
    "k_values = [3, 5, 7, 9]\n",
    "\n",
    "# Validation 예측\n",
    "val_predictions = ensemble_predictions(val_scaled, k_values)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = ensemble_predictions(test_scaled, k_values)\n",
    "\n",
    "# 결과 저장\n",
    "submission_df = pd.DataFrame({\n",
    "    'index': test_df['index'] if 'index' in test_df.columns else range(len(test_predictions)),\n",
    "    'deposit': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")\n",
    "\n",
    "# 특성 중요도 분석\n",
    "# 1. 상관관계 기반 중요도\n",
    "correlation_importance = train_df[features + ['deposit']].corr()['deposit'].abs().sort_values(ascending=False)\n",
    "\n",
    "# 2. 랜덤 포레스트 기반 중요도\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(train_scaled, train_df['deposit'])\n",
    "rf_importance = pd.Series(rf_model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n상관관계 기반 특성 중요도:\")\n",
    "print(correlation_importance)\n",
    "\n",
    "print(\"\\n랜덤 포레스트 기반 특성 중요도:\")\n",
    "print(rf_importance)\n",
    "\n",
    "# 중요도 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "correlation_importance[:10].plot(kind='bar')\n",
    "plt.title('Top 10 Features (Correlation-based Importance)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_importance.png')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "rf_importance[:10].plot(kind='bar')\n",
    "plt.title('Top 10 Features (Random Forest Importance)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rf_importance.png')\n",
    "\n",
    "print(\"특성 중요도 그래프가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 5591.6022472139475\n",
      "예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 로드 및 샘플링\n",
    "merged_data = pd.read_csv('merged_data_cleaned.csv')\n",
    "\n",
    "# Train과 Test 데이터 분리\n",
    "train_df = merged_data[merged_data['deposit'] != 0].copy()\n",
    "test_df = merged_data[merged_data['deposit'] == 0].copy()\n",
    "\n",
    "# 특성 선택 (중요도가 높은 10개 특성만 선택)\n",
    "features = ['area_m2', 'latitude', 'longitude', 'floor', 'built_year', 'contract_year_month', \n",
    "            'nearest_subway_distance_km', 'deposit_mean', 'interest_rate', 'school_count_within_1km']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# KNN 모델 생성\n",
    "k = 5  # 단일 k 값 사용\n",
    "knn = KNeighborsRegressor(n_neighbors=k, weights='distance', metric='euclidean', n_jobs=-1)\n",
    "knn.fit(train_scaled, train_df['deposit'])\n",
    "\n",
    "# Validation 예측\n",
    "val_predictions = knn.predict(val_scaled)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = knn.predict(test_scaled)\n",
    "\n",
    "# 결과 저장\n",
    "submission_df = pd.DataFrame({\n",
    "    'index': test_df['index'] if 'index' in test_df.columns else range(len(test_predictions)),\n",
    "    'deposit': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:25153.55059\n",
      "[100]\tvalidation_0-rmse:9157.11119\n",
      "[200]\tvalidation_0-rmse:8450.68627\n",
      "[300]\tvalidation_0-rmse:8132.15768\n",
      "[400]\tvalidation_0-rmse:7918.70562\n",
      "[499]\tvalidation_0-rmse:7769.50328\n",
      "Validation MAE: 4714.424762007194\n",
      "예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\n",
      "\n",
      "상위 10개 중요 특성:\n",
      "                       feature  importance\n",
      "24  total_large_park_area_10km    0.172603\n",
      "0                      area_m2    0.128057\n",
      "6                     latitude    0.110311\n",
      "7                    longitude    0.085020\n",
      "23       large_park_count_10km    0.082671\n",
      "5                   built_year    0.081762\n",
      "25  nearest_subway_distance_km    0.041096\n",
      "30                deposit_mean    0.024595\n",
      "8                          age    0.022970\n",
      "21        large_park_count_3km    0.020564\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# 데이터 로드\n",
    "merged_data = pd.read_csv('merged_data_cleaned.csv')\n",
    "\n",
    "# Train과 Test 데이터 분리\n",
    "train_df = merged_data[merged_data['deposit'] != 0].copy()\n",
    "test_df = merged_data[merged_data['deposit'] == 0].copy()\n",
    "\n",
    "# 타겟 변수 분리\n",
    "y_train = train_df['deposit']\n",
    "X_train = train_df.drop('deposit', axis=1)\n",
    "\n",
    "# test_df에서 deposit 열 제거\n",
    "X_test = test_df.drop('deposit', axis=1)\n",
    "\n",
    "# 범주형 변수 인코딩\n",
    "le = LabelEncoder()\n",
    "for col in X_train.select_dtypes(include=['object']).columns:\n",
    "    X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "    X_test[col] = le.transform(X_test[col].astype(str))\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost 모델 생성 및 학습\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6, \n",
    "    subsample=0.8, \n",
    "    colsample_bytree=0.8, \n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, \n",
    "    eval_set=[(X_val, y_val)], \n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Validation 예측 및 MAE 계산\n",
    "val_predictions = model.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# 결과 저장\n",
    "submission_df = pd.DataFrame({\n",
    "    'index': test_df.index,\n",
    "    'deposit': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")\n",
    "\n",
    "# 특성 중요도 출력\n",
    "feature_importance = model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "print(\"\\n상위 10개 중요 특성:\")\n",
    "print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블용 knn모델 베이스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 5192.85860614596\n",
      "예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 데이터 로드\n",
    "merged_data = pd.read_csv('merged_data_cleaned.csv')\n",
    "\n",
    "# Train과 Test 데이터 분리\n",
    "train_df = merged_data[merged_data['deposit'] != 0].copy()\n",
    "test_df = merged_data[merged_data['deposit'] == 0].copy()\n",
    "\n",
    "# 특성 선택\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "\n",
    "# Train 데이터를 train과 validation으로 분리\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 특성 가중치 조정\n",
    "weights = np.array([1, 1, 5, 0.5, 0.5, 0.5])  # latitude, longitude, area_m2, floor, built_year, contract_year_month 순서\n",
    "train_scaled = train_scaled * weights\n",
    "val_scaled = val_scaled * weights\n",
    "test_scaled = test_scaled * weights\n",
    "\n",
    "# BallTree 구축 (맨해튼 거리 사용)\n",
    "tree = BallTree(train_scaled, leaf_size=40, metric='manhattan')\n",
    "\n",
    "# 예측 함수 (거리에 따른 가중치 함수 조정)\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = np.exp(-distances)  # 지수 함수 사용\n",
    "    weighted_deposits = neighbors['deposit'] * weights\n",
    "    return np.sum(weighted_deposits) / np.sum(weights)\n",
    "\n",
    "# 단일 예측 함수\n",
    "def predict_single(data_point, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수\n",
    "def make_predictions_parallel(data_scaled, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, k) for data_point in data_scaled)\n",
    "\n",
    "# 앙상블 예측 함수\n",
    "def ensemble_predictions(data_scaled, k_values):\n",
    "    all_predictions = Parallel(n_jobs=-1)(\n",
    "        delayed(make_predictions_parallel)(data_scaled, k) for k in k_values\n",
    "    )\n",
    "    return np.mean(all_predictions, axis=0)\n",
    "\n",
    "# k 값 리스트 정의\n",
    "k_values = [3, 5, 7, 9]\n",
    "\n",
    "# Validation 예측\n",
    "val_predictions = ensemble_predictions(val_scaled, k_values)\n",
    "\n",
    "# Validation MAE 계산\n",
    "val_mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = ensemble_predictions(test_scaled, k_values)\n",
    "\n",
    "# 결과 저장\n",
    "submission_df = pd.DataFrame({\n",
    "    'index': test_df.index,\n",
    "    'deposit': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블용 knn 그리드 서치중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100, Weights: [0.5, 2, 0.5, 5, 5, 5], MAE: 6653.438921437086\n",
      "Iteration 2/100, Weights: [0.1, 2, 0.5, 0.1, 10, 5], MAE: 5286.464545782188\n",
      "Iteration 3/100, Weights: [2, 0.5, 0.1, 1, 5, 10], MAE: 6443.017427882524\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# 데이터 로드 및 전처리 (이전과 동일)\n",
    "merged_data = pd.read_csv('merged_data_cleaned.csv')\n",
    "train_df = merged_data[merged_data['deposit'] != 0].copy()\n",
    "test_df = merged_data[merged_data['deposit'] == 0].copy()\n",
    "features = ['latitude', 'longitude', 'area_m2', 'floor', 'built_year', 'contract_year_month']\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "val_scaled = scaler.transform(val_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# 예측 함수 최적화\n",
    "def predict_deposit(neighbors, distances):\n",
    "    weights = np.exp(-distances)\n",
    "    return np.average(neighbors['deposit'], weights=weights)\n",
    "\n",
    "# 단일 예측 함수 (변경 없음)\n",
    "def predict_single(data_point, tree, k):\n",
    "    distances, indices = tree.query([data_point], k=k)\n",
    "    neighbors = train_df.iloc[indices[0]]\n",
    "    return predict_deposit(neighbors, distances[0])\n",
    "\n",
    "# 병렬 예측 수행 함수 (변경 없음)\n",
    "def make_predictions_parallel(data_scaled, tree, k):\n",
    "    return Parallel(n_jobs=-1)(delayed(predict_single)(data_point, tree, k) for data_point in data_scaled)\n",
    "\n",
    "# 앙상블 예측 함수 (변경 없음)\n",
    "def ensemble_predictions(data_scaled, tree, k_values):\n",
    "    all_predictions = Parallel(n_jobs=-1)(\n",
    "        delayed(make_predictions_parallel)(data_scaled, tree, k) for k in k_values\n",
    "    )\n",
    "    return np.mean(all_predictions, axis=0)\n",
    "\n",
    "# 최적화된 그리드 서치 함수\n",
    "def optimized_grid_search(train_scaled, val_scaled, val_df, k_values, max_iterations=100):\n",
    "    best_mae = float('inf')\n",
    "    best_weights = None\n",
    "    \n",
    "    # 가중치 범위 설정\n",
    "    weight_options = [0.1, 0.5, 1, 2, 5, 10]\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        # 무작위로 가중치 선택\n",
    "        weights = [random.choice(weight_options) for _ in range(len(features))]\n",
    "        \n",
    "        # 가중치 적용\n",
    "        train_weighted = train_scaled * weights\n",
    "        val_weighted = val_scaled * weights\n",
    "        \n",
    "        # BallTree 구축\n",
    "        tree = BallTree(train_weighted, leaf_size=40, metric='manhattan')\n",
    "        \n",
    "        # 예측 및 MAE 계산\n",
    "        val_predictions = ensemble_predictions(val_weighted, tree, k_values)\n",
    "        mae = mean_absolute_error(val_df['deposit'], val_predictions)\n",
    "        \n",
    "        # 최적의 가중치 업데이트\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_weights = weights\n",
    "        \n",
    "        print(f\"Iteration {_+1}/{max_iterations}, Weights: {weights}, MAE: {mae}\")\n",
    "    \n",
    "    return best_weights, best_mae\n",
    "\n",
    "# k 값 리스트 정의\n",
    "k_values = [3, 5, 7]  # k 값 감소\n",
    "\n",
    "# 최적화된 그리드 서치 수행\n",
    "best_weights, best_mae = optimized_grid_search(train_scaled, val_scaled, val_df, k_values, max_iterations=100)\n",
    "\n",
    "print(f\"Best weights: {best_weights}\")\n",
    "print(f\"Best Validation MAE: {best_mae}\")\n",
    "\n",
    "# 최적의 가중치 적용\n",
    "train_scaled = train_scaled * best_weights\n",
    "test_scaled = test_scaled * best_weights\n",
    "\n",
    "# 최종 모델 학습\n",
    "final_tree = BallTree(train_scaled, leaf_size=40, metric='manhattan')\n",
    "\n",
    "# Test 예측\n",
    "test_predictions = ensemble_predictions(test_scaled, final_tree, k_values)\n",
    "\n",
    "# 결과 저장\n",
    "submission_df = pd.DataFrame({\n",
    "    'index': test_df.index,\n",
    "    'deposit': test_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 'submission.csv' 파일을 확인하세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
